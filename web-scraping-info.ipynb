{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping article texts\n",
    "\n",
    "In this notebook, I get a list of article urls and then iterate over it to get over 6000 article texts. Each text is then stored as a separate line in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from numpy import random\n",
    "from time import sleep\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "headers={'User-Agent':user_agent,} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_from_url(url):\n",
    "    request = Request(url,None,headers) \n",
    "    response_soup = urlopen(request)\n",
    "    soup = BeautifulSoup(response_soup, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def get_links_from_soup(soup, key_phrase):\n",
    "    hrefs = [link.get('href') for link in soup.findAll('a')]\n",
    "    relevant_links = list(set([h for h in hrefs if h is not None and key_phrase in h]))\n",
    "    return relevant_links\n",
    "\n",
    "def get_links_from_url(url, key_phrase):\n",
    "    soup = get_soup_from_url(url)\n",
    "    links =  get_links_from_soup(soup, key_phrase)\n",
    "    return links\n",
    "\n",
    "def get_links_from_url_and_sleep(url, key_phrase):\n",
    "    links = get_links_from_url(url, key_phrase)\n",
    "    sleep(random.uniform(0, 2))\n",
    "    return links\n",
    "\n",
    "def get_all_links(url_beginning, max_page = 150, key_phrase = \"/news/\"):\n",
    "    all_links = []\n",
    "    for i in range(1, max_page+1):\n",
    "        links = get_links_from_url_and_sleep(url_beginning+str(i), key_phrase)\n",
    "        all_links += links\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lrt's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def construct_url(page_nr, start_date, end_date, category_id):\n",
    "    return \"https://www.lrt.lt/api/search?page=\"+str(page_nr)+\"&count=100&dfrom=\"+start_date+\"&dto=\"+end_date+\"&order=desc&type=1&category_id=\"+str(category_id)\n",
    "\n",
    "def get_df_from_url(url):\n",
    "    data = requests.get(url).json()\n",
    "    df = pd.DataFrame(data[\"items\"])\n",
    "    return df\n",
    "\n",
    "def get_df_from_page(page_nr, start_date, end_date, category_id):\n",
    "    url = construct_url(page_nr, start_date, end_date, category_id)\n",
    "    df = get_df_from_url(url)\n",
    "    return df\n",
    "\n",
    "def get_df_from_page_and_sleep(page_nr, start_date, end_date, category_id):\n",
    "    df = get_df_from_page(page_nr, start_date, end_date, category_id)\n",
    "    sleep(random.uniform(0, 2))\n",
    "    return df\n",
    "\n",
    "def get_all_results(start_date, end_date, category_id):\n",
    "\n",
    "    first_page_url = construct_url(page_nr=1, start_date=start_date, end_date=end_date, category_id=category_id)\n",
    "    data = requests.get(first_page_url).json()\n",
    "    total_found = int(data[\"total_found\"])\n",
    "    max_page = math.ceil(total_found/100)\n",
    "\n",
    "    print(f\"Total entries available: {str(total_found)}\\nLargest page number: {max_page}\")\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for i in range(1, max_page+1):\n",
    "        df = get_df_from_page_and_sleep(i, start_date, end_date, category_id)\n",
    "        dfs.append(df)\n",
    "\n",
    "    result_df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def get_article_text(url):\n",
    "    request = Request(url,None,headers) \n",
    "    response_soup = urlopen(request)\n",
    "    soup = BeautifulSoup(response_soup, \"html.parser\")\n",
    "    text = \"\".join([elem.get_text(strip=True) for elem in soup.find_all(\"p\")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the article urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries available: 11660\n",
      "Largest page number: 117\n"
     ]
    }
   ],
   "source": [
    "start_date=\"2020-01-01\"\n",
    "end_date=\"2024-04-20\"\n",
    "category_id=19\n",
    "\n",
    "result_df = get_all_results(start_date=start_date, end_date=end_date, category_id=category_id)\n",
    "result_df[\"full_url\"] = \"https://www.lrt.lt\"+result_df[\"url\"]\n",
    "\n",
    "#result_df.to_csv(\"C:/Users/Ugne/Documents/studies/Python/DL-task1/deep-learning-task-2/lrt_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the text for every article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for url in result_df[\"full_url\"]:\n",
    "    result = get_article_text(url)\n",
    "    texts.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6160"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(\n",
    "    {\"text\": list(set(texts))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv(\"C:/Users/Ugne/Documents/studies/Python/DL-task1/deep-learning-task-2/lrt_article_texts.csv\", index=False, header=False, encoding=\"utf-8\", errors=\"replace\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
